{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eba16cc",
   "metadata": {},
   "source": [
    "# Label-Free Concept Bottleneck Model (LF-CBM) for Brain Scan Classification\n",
    "\n",
    "This notebook implements an explainable AI approach using BioMedCLIP to classify brain scans based on clinical concepts rather than raw pixels.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load BioMedCLIP pre-trained model\n",
    "2. Define clinical concept bank\n",
    "3. Extract image-text similarity scores\n",
    "4. Train logistic regression on concept scores\n",
    "5. Analyze concept importance for explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad27b5",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "First, let's install the necessary packages for BioMedCLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1039cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchvision pillow scikit-learn open_clip_torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2c88c",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec09c9",
   "metadata": {},
   "source": [
    "## Step 3: Load BioMedCLIP Model\n",
    "\n",
    "Load the pre-trained BioMedCLIP model from Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BioMedCLIP model...\")\n",
    "model_name = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "# Load model, tokenizer, and image processor\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ“ BioMedCLIP model loaded successfully on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7f27c",
   "metadata": {},
   "source": [
    "## Step 4: Define Clinical Concept Bank\n",
    "\n",
    "Define the clinical concepts for each condition that will serve as our interpretable features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca2d9a",
   "metadata": {},
   "source": [
    "### Dataset Visual Analysis Results\n",
    "\n",
    "Before defining concepts, let's review the visual characteristics of your dataset:\n",
    "\n",
    "**Hemorrhagic Images:**\n",
    "- Primary sequences: SWI (77%), DWI (14%), GRE (8%)\n",
    "- Average brightness: 0.258 (moderate)\n",
    "- Key features: Dark signals, blooming artifacts, well-defined boundaries\n",
    "\n",
    "**Ischemic Images:**\n",
    "- Primary sequences: Mixed/DWI-based\n",
    "- Average brightness: 0.194 (darkest class)\n",
    "- Key features: Territorial patterns, diffusion restriction, wedge-shaped lesions\n",
    "\n",
    "**Tumor Images:**\n",
    "- Various MRI sequences\n",
    "- Average brightness: 0.301 (brightest class)\n",
    "- Key features: Mass effect, irregular margins, heterogeneous signals\n",
    "\n",
    "These observations inform our concept selection below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27841f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clinical concepts for each class\n",
    "# These concepts are specifically tailored to this dataset based on visual analysis\n",
    "# Dataset characteristics:\n",
    "# - Hemorrhagic: 77% SWI, 14% DWI, 8% GRE | Brightness: 0.258, Contrast: 0.313\n",
    "# - Ischemic: Mixed sequences (73% unlabeled), 17% DWI | Brightness: 0.194 (darkest), Contrast: 0.225\n",
    "# - Tumor: Various contrasts | Brightness: 0.301 (brightest), Edge Density: 0.0414 (highest)\n",
    "\n",
    "concept_bank = {\n",
    "    \"Tumor\": [\n",
    "        \"space-occupying lesion\",\n",
    "        \"mass effect present\",\n",
    "        \"irregular lesion margins\",\n",
    "        \"vasogenic edema pattern\",\n",
    "        \"central hypointensity\",\n",
    "        \"heterogeneous signal intensity\",\n",
    "        \"fingerlike edema extension\",\n",
    "        \"structural displacement\",\n",
    "        \"necrotic center\"\n",
    "    ],\n",
    "    \"Hemorrhagic\": [\n",
    "        \"dark signal on susceptibility weighted imaging\",\n",
    "        \"blooming artifact present\",\n",
    "        \"focal hypointense region\",\n",
    "        \"blood products signal intensity\",\n",
    "        \"susceptibility artifact\",\n",
    "        \"well-circumscribed hemorrhagic lesion\",\n",
    "        \"acute bleeding pattern\",\n",
    "        \"hypointense on SWI\"\n",
    "    ],\n",
    "    \"Ischemic\": [\n",
    "        \"bright signal on diffusion weighted imaging\",\n",
    "        \"territorial distribution pattern\",\n",
    "        \"vascular territory involvement\",\n",
    "        \"diffusion restriction present\",\n",
    "        \"wedge-shaped lesion\",\n",
    "        \"cortical gray matter involvement\",\n",
    "        \"hypointense region on apparent diffusion coefficient\",\n",
    "        \"acute infarct pattern\"\n",
    "    ],\n",
    "    \"Normal\": [\n",
    "        \"normal gray matter signal\",\n",
    "        \"normal white matter signal\",\n",
    "        \"symmetric brain structures\",\n",
    "        \"no abnormal signal intensity\",\n",
    "        \"preserved brain anatomy\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten all concepts into a single list\n",
    "all_concepts = []\n",
    "concept_to_class = {}\n",
    "for class_name, concepts in concept_bank.items():\n",
    "    for concept in concepts:\n",
    "        all_concepts.append(concept)\n",
    "        concept_to_class[concept] = class_name\n",
    "\n",
    "print(f\"Total concepts: {len(all_concepts)}\")\n",
    "print(\"\\nDataset-Specific Concept Bank:\")\n",
    "print(\"(Based on analysis of your processed images)\\n\")\n",
    "for class_name, concepts in concept_bank.items():\n",
    "    print(f\"\\n{class_name} ({len(concepts)} concepts):\")\n",
    "    for concept in concepts:\n",
    "        print(f\"  - {concept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11070d3a",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Text Embeddings\n",
    "\n",
    "Pre-compute text embeddings for all concepts to speed up the feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351136fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing text embeddings for all concepts...\")\n",
    "\n",
    "# Tokenize all concepts\n",
    "text_inputs = tokenizer(all_concepts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "# Get text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.get_text_features(**text_inputs)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "\n",
    "print(f\"âœ“ Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"  ({text_embeddings.shape[0]} concepts Ã— {text_embeddings.shape[1]} dimensions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada62ca",
   "metadata": {},
   "source": [
    "## Step 6: Load Dataset\n",
    "\n",
    "Load all processed images from the dataset directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "data_dir = Path(\"../content/processed_data\")\n",
    "\n",
    "# Class mapping\n",
    "class_names = [\"Hemorrhagic\", \"Ischemic\", \"Tumor\"]\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Load image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dir = data_dir / class_name\n",
    "    if not class_dir.exists():\n",
    "        print(f\"Warning: Directory {class_dir} not found!\")\n",
    "        continue\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        image_paths.append(str(img_path))\n",
    "        labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    print(f\"{class_name}: {len(image_files)} images\")\n",
    "\n",
    "print(f\"\\nTotal images: {len(image_paths)}\")\n",
    "print(f\"Labels distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbd6a4",
   "metadata": {},
   "source": [
    "## Step 7: Extract Image-Text Similarity Features\n",
    "\n",
    "For each image, compute similarity scores with all clinical concepts using BioMedCLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7feb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_concept_features(image_path, text_embeddings, model, image_processor, device):\n",
    "    \"\"\"\n",
    "    Extract concept-based features for a single image.\n",
    "    Returns similarity scores between the image and all concepts.\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get image embedding\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.get_image_features(**inputs)\n",
    "        image_embedding = F.normalize(image_embedding, dim=-1)\n",
    "    \n",
    "    # Compute similarity with all concepts (cosine similarity via dot product)\n",
    "    similarities = (image_embedding @ text_embeddings.T).squeeze(0)\n",
    "    \n",
    "    return similarities.cpu().numpy()\n",
    "\n",
    "# Extract features for all images\n",
    "print(\"Extracting concept features from images...\")\n",
    "print(\"This may take a few minutes depending on dataset size...\\n\")\n",
    "\n",
    "X_features = []\n",
    "y_labels = []\n",
    "\n",
    "for img_path, label in tqdm(zip(image_paths, labels), total=len(image_paths), desc=\"Processing images\"):\n",
    "    try:\n",
    "        features = extract_concept_features(img_path, text_embeddings, model, image_processor, device)\n",
    "        X_features.append(features)\n",
    "        y_labels.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X_features)\n",
    "y = np.array(y_labels)\n",
    "\n",
    "print(f\"\\nâœ“ Feature extraction complete!\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"  ({X.shape[0]} images Ã— {X.shape[1]} concept scores)\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512bf2a1",
   "metadata": {},
   "source": [
    "## Step 8: Train-Test Split\n",
    "\n",
    "Split the concept features into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af050574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} images\")\n",
    "print(f\"Testing set: {X_test.shape[0]} images\")\n",
    "print(f\"\\nTraining labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing labels distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c82cc",
   "metadata": {},
   "source": [
    "## Step 9: Train Concept Bottleneck Model\n",
    "\n",
    "Train a logistic regression classifier on the concept scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be01cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression on concept scores...\\n\")\n",
    "\n",
    "# Train logistic regression classifier\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"âœ“ Training complete!\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"MODEL PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1caba2",
   "metadata": {},
   "source": [
    "## Step 10: Detailed Classification Report\n",
    "\n",
    "View detailed metrics for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_test_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dc660",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Confusion Matrix\n",
    "\n",
    "Create a visual representation of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - LF-CBM Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Normalized Confusion Matrix - LF-CBM Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70264f",
   "metadata": {},
   "source": [
    "## Step 12: Explainability - Concept Importance Analysis\n",
    "\n",
    "Analyze which concepts are most important for each class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from logistic regression\n",
    "weights = clf.coef_  # Shape: (n_classes, n_concepts)\n",
    "print(f\"Model weights shape: {weights.shape}\")\n",
    "print(f\"  ({weights.shape[0]} classes Ã— {weights.shape[1]} concepts)\\n\")\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "concept_importance = pd.DataFrame(\n",
    "    weights.T,\n",
    "    columns=class_names,\n",
    "    index=all_concepts\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONCEPT WEIGHTS FOR EACH CLASS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPositive weights indicate concepts that support the class prediction.\")\n",
    "print(\"Negative weights indicate concepts that oppose the class prediction.\\n\")\n",
    "\n",
    "for class_name in class_names:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TOP CONCEPTS FOR: {class_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Sort concepts by absolute weight\n",
    "    sorted_concepts = concept_importance[class_name].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Concepts (by absolute weight):\\n\")\n",
    "    for i, (concept, _) in enumerate(sorted_concepts.head(10).items(), 1):\n",
    "        weight = concept_importance.loc[concept, class_name]\n",
    "        impact = \"Supporting\" if weight > 0 else \"Opposing\"\n",
    "        print(f\"{i:2d}. {concept:35s} | Weight: {weight:+7.4f} | {impact}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(\"Top 5 Supporting Concepts (highest positive weights):\\n\")\n",
    "    top_positive = concept_importance[class_name].sort_values(ascending=False).head(5)\n",
    "    for i, (concept, weight) in enumerate(top_positive.items(), 1):\n",
    "        print(f\"{i}. {concept:35s} | Weight: {weight:+7.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(\"Top 5 Opposing Concepts (most negative weights):\\n\")\n",
    "    top_negative = concept_importance[class_name].sort_values(ascending=True).head(5)\n",
    "    for i, (concept, weight) in enumerate(top_negative.items(), 1):\n",
    "        print(f\"{i}. {concept:35s} | Weight: {weight:+7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c799d3",
   "metadata": {},
   "source": [
    "## Step 13: Visualize Concept Importance\n",
    "\n",
    "Create heatmaps to visualize which concepts are important for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebe6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of all concept weights\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(concept_importance, cmap='RdBu_r', center=0, \n",
    "            annot=True, fmt='.3f', linewidths=0.5,\n",
    "            cbar_kws={'label': 'Weight'})\n",
    "plt.title('Concept Importance Heatmap\\n(Weights from Logistic Regression)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "plt.ylabel('Clinical Concepts', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for each class\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get top 10 concepts by absolute weight\n",
    "    top_concepts = concept_importance[class_name].abs().sort_values(ascending=False).head(10)\n",
    "    top_concepts_signed = concept_importance.loc[top_concepts.index, class_name]\n",
    "    \n",
    "    # Create color based on sign\n",
    "    colors = ['green' if w > 0 else 'red' for w in top_concepts_signed]\n",
    "    \n",
    "    # Plot\n",
    "    top_concepts_signed.sort_values().plot(kind='barh', ax=ax, color=colors, alpha=0.7)\n",
    "    ax.set_title(f'Top 10 Concepts for {class_name}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Weight', fontsize=10)\n",
    "    ax.set_ylabel('Concept', fontsize=10)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ff546",
   "metadata": {},
   "source": [
    "## Step 14: Example Prediction with Explanation\n",
    "\n",
    "Demonstrate how to make a prediction with concept-based explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prediction(image_path, model, image_processor, text_embeddings, \n",
    "                      clf, all_concepts, class_names, device, top_k=5):\n",
    "    \"\"\"\n",
    "    Make a prediction and explain it using concept scores.\n",
    "    \"\"\"\n",
    "    # Extract concept features\n",
    "    features = extract_concept_features(image_path, text_embeddings, model, \n",
    "                                       image_processor, device)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = clf.predict(features.reshape(1, -1))[0]\n",
    "    probabilities = clf.predict_proba(features.reshape(1, -1))[0]\n",
    "    predicted_class = class_names[prediction]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"PREDICTION EXPLANATION FOR: {image_path}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nðŸ” Predicted Class: {predicted_class}\")\n",
    "    print(f\"\\nClass Probabilities:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        prob = probabilities[i]\n",
    "        bar = 'â–ˆ' * int(prob * 50)\n",
    "        print(f\"  {class_name:15s}: {prob:.4f} ({prob*100:.2f}%) {bar}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONCEPT ACTIVATION SCORES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\\nHighest Activated Concepts (indicating presence):\\n\")\n",
    "    \n",
    "    # Get top activated concepts\n",
    "    concept_scores = pd.Series(features, index=all_concepts)\n",
    "    top_activated = concept_scores.sort_values(ascending=False).head(top_k)\n",
    "    \n",
    "    for i, (concept, score) in enumerate(top_activated.items(), 1):\n",
    "        # Get weight for predicted class\n",
    "        concept_idx = all_concepts.index(concept)\n",
    "        weight = clf.coef_[prediction, concept_idx]\n",
    "        contribution = score * weight\n",
    "        \n",
    "        print(f\"{i}. {concept:35s}\")\n",
    "        print(f\"   Similarity Score: {score:.4f}\")\n",
    "        print(f\"   Weight for {predicted_class}: {weight:+.4f}\")\n",
    "        print(f\"   Contribution: {contribution:+.4f}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"INTERPRETATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nThe model predicted '{predicted_class}' based on the following reasoning:\")\n",
    "    print(f\"\\nTop supporting evidence:\")\n",
    "    \n",
    "    # Find concepts with high score AND high positive weight for predicted class\n",
    "    contributions = []\n",
    "    for i, concept in enumerate(all_concepts):\n",
    "        score = features[i]\n",
    "        weight = clf.coef_[prediction, i]\n",
    "        contrib = score * weight\n",
    "        contributions.append((concept, score, weight, contrib))\n",
    "    \n",
    "    # Sort by contribution\n",
    "    contributions.sort(key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "    for i, (concept, score, weight, contrib) in enumerate(contributions[:3], 1):\n",
    "        if contrib > 0:\n",
    "            print(f\"{i}. Detected '{concept}' (similarity: {score:.3f}, weight: {weight:+.3f})\")\n",
    "    \n",
    "    return prediction, probabilities, features\n",
    "\n",
    "# Test on a random sample from test set\n",
    "test_idx = np.random.randint(0, len(X_test))\n",
    "test_image_path = image_paths[test_idx]\n",
    "true_label = class_names[y[test_idx]]\n",
    "\n",
    "print(f\"Testing on image from: {test_image_path}\")\n",
    "print(f\"True label: {true_label}\\n\")\n",
    "\n",
    "prediction, probs, features = explain_prediction(\n",
    "    test_image_path, model, image_processor, text_embeddings,\n",
    "    clf, all_concepts, class_names, device, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca194e",
   "metadata": {},
   "source": [
    "## Step 15: Save the Model\n",
    "\n",
    "Save the trained model and concept bank for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save the logistic regression model\n",
    "model_path = \"../models/lfcbm_classifier.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "print(f\"âœ“ Saved classifier to: {model_path}\")\n",
    "\n",
    "# Save the concept bank and metadata\n",
    "metadata = {\n",
    "    \"concept_bank\": concept_bank,\n",
    "    \"all_concepts\": all_concepts,\n",
    "    \"class_names\": class_names,\n",
    "    \"class_to_idx\": class_to_idx,\n",
    "    \"model_name\": model_name,\n",
    "    \"test_accuracy\": float(test_acc),\n",
    "    \"train_accuracy\": float(train_acc)\n",
    "}\n",
    "\n",
    "metadata_path = \"../models/lfcbm_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"âœ“ Saved metadata to: {metadata_path}\")\n",
    "\n",
    "# Save concept importance\n",
    "importance_path = \"../models/lfcbm_concept_importance.csv\"\n",
    "concept_importance.to_csv(importance_path)\n",
    "print(f\"âœ“ Saved concept importance to: {importance_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nYou can now load and use this model for inference with explainability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afe9dd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a **Label-Free Concept Bottleneck Model (LF-CBM)** for explainable brain scan classification:\n",
    "\n",
    "### Key Components:\n",
    "1. **BioMedCLIP**: Pre-trained medical vision-language model\n",
    "2. **Concept Bank**: Clinical concepts for Tumor, Hemorrhagic, Ischemic, and Normal brain\n",
    "3. **Feature Extraction**: Image-text similarity scores as interpretable features\n",
    "4. **Classification**: Logistic Regression on concept scores\n",
    "5. **Explainability**: Concept weights show which clinical features drive predictions\n",
    "\n",
    "### Benefits:\n",
    "- âœ… **Interpretable**: Predictions explained by clinical concepts\n",
    "- âœ… **Transparent**: See which concepts contribute to each decision\n",
    "- âœ… **Medical-relevant**: Uses domain-specific concepts from BioMedCLIP\n",
    "- âœ… **No manual labeling**: Label-free approach using vision-language alignment\n",
    "\n",
    "### How to Use:\n",
    "The model can now predict with explanations like:\n",
    "> \"Predicted **Tumor** because:\n",
    "> - High 'mass effect' score (0.85)\n",
    "> - High 'midline shift' score (0.78)\n",
    "> - High 'edema surrounding mass' score (0.72)\"\n",
    "\n",
    "This provides clinically meaningful insights into the model's decision-making process!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
