{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d915595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  initializing Locator U-Net...\n",
      "ðŸš€ Training Locator on 3163 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.1202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.1131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0939\n",
      "âœ… Locator U-Net Saved as 'locator_unet.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "RAW_DATA_DIR = \"../content/dataset\" # Point to RAW data\n",
    "CLASSES = [\"Hemorrhagic\", \"Ischemic\", \"Tumor\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5 # ROI extraction is easy, 5 epochs is enough\n",
    "TARGET_SIZE = (224, 224)\n",
    "\n",
    "# --- 2. THE TEACHER (Robust CV2 Function) ---\n",
    "def generate_ground_truth_mask(image_np):\n",
    "    \"\"\"\n",
    "    Uses our robust CV2 logic to create a binary mask (0 or 1)\n",
    "    to train the U-Net.\n",
    "    \"\"\"\n",
    "    img_copy = image_np.copy()\n",
    "    img_h, img_w = img_copy.shape[:2]\n",
    "    gray = cv2.cvtColor(img_copy, cv2.COLOR_RGB2GRAY) # Note: RGB input from PIL\n",
    "    \n",
    "    # Blur & Threshold\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Dilate\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    dilated = cv2.dilate(thresh, kernel, iterations=3)\n",
    "    \n",
    "    # Filter Contours (Remove Text)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    valid_contours = []\n",
    "    if contours:\n",
    "        for c in contours:\n",
    "            if cv2.contourArea(c) < (0.01 * img_h * img_w): continue # Tiny noise\n",
    "            \n",
    "            # Positional Filter (Ignore Header/Footer text)\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            cy = y + h/2\n",
    "            if (cy < img_h * 0.10) or (cy > img_h * 0.90): continue\n",
    "            \n",
    "            valid_contours.append(c)\n",
    "            \n",
    "    # Create Mask\n",
    "    mask = np.zeros_like(gray, dtype=np.float32)\n",
    "    if valid_contours:\n",
    "        c = max(valid_contours, key=cv2.contourArea)\n",
    "        cv2.drawContours(mask, [c], -1, 1.0, -1) # 1.0 for Brain\n",
    "        \n",
    "    return mask\n",
    "\n",
    "# --- 3. DATASET (Generates Masks on the Fly) ---\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.files = []\n",
    "        for cls in CLASSES:\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.exists(cls_dir): continue\n",
    "            for f in os.listdir(cls_dir):\n",
    "                if f.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    self.files.append(os.path.join(cls_dir, f))\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        \n",
    "        # Load Image\n",
    "        try:\n",
    "            img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "            img_np = np.array(img_pil)\n",
    "        except:\n",
    "            img_pil = Image.new('RGB', TARGET_SIZE)\n",
    "            img_np = np.array(img_pil)\n",
    "            \n",
    "        # Generate Target Mask (The Teacher)\n",
    "        mask = generate_ground_truth_mask(img_np)\n",
    "        \n",
    "        # Resize both to target\n",
    "        img_pil = img_pil.resize(TARGET_SIZE)\n",
    "        mask_pil = Image.fromarray((mask * 255).astype(np.uint8)) # Convert to PIL for resize\n",
    "        mask_pil = mask_pil.resize(TARGET_SIZE, resample=Image.NEAREST)\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        img_t = transforms.ToTensor()(img_pil) # (3, 224, 224)\n",
    "        mask_t = transforms.ToTensor()(mask_pil) # (1, 224, 224)\n",
    "        \n",
    "        return img_t, mask_t\n",
    "\n",
    "# --- 4. MODEL (Simple U-Net) ---\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.e1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.e2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.Conv2d(128, 128, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.b = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.Conv2d(256, 256, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Decoder\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.d2 = nn.Sequential(nn.Conv2d(256 + 128, 128, 3, padding=1), nn.ReLU(), nn.Conv2d(128, 128, 3, padding=1), nn.ReLU())\n",
    "        self.d1 = nn.Sequential(nn.Conv2d(128 + 64, 64, 3, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(64, 1, 1) # 1 Channel Output (Mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.e1(x)\n",
    "        p1 = self.pool(x1)\n",
    "        x2 = self.e2(p1)\n",
    "        p2 = self.pool(x2)\n",
    "        \n",
    "        b = self.b(p2)\n",
    "        \n",
    "        u2 = self.up(b)\n",
    "        u2 = torch.cat([u2, x2], dim=1) # Skip Connection\n",
    "        x3 = self.d2(u2)\n",
    "        \n",
    "        u1 = self.up(x3)\n",
    "        u1 = torch.cat([u1, x1], dim=1) # Skip Connection\n",
    "        x4 = self.d1(u1)\n",
    "        \n",
    "        return torch.sigmoid(self.out(x4))\n",
    "\n",
    "# --- 5. TRAINING LOOP ---\n",
    "def train_locator_unet():\n",
    "    print(\"ðŸ§  initializing Locator U-Net...\")\n",
    "    dataset = SegmentationDataset(RAW_DATA_DIR)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    model = SimpleUNet().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss() # Binary Cross Entropy for Masks\n",
    "    \n",
    "    print(f\"ðŸš€ Training Locator on {len(dataset)} images...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Ep {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, masks in loop:\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(loader):.4f}\")\n",
    "        \n",
    "    # Save the Locator\n",
    "    torch.save(model.state_dict(), \"locator_unet.pth\")\n",
    "    print(\"âœ… Locator U-Net Saved as 'locator_unet.pth'\")\n",
    "\n",
    "# RUN IT\n",
    "train_locator_unet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
